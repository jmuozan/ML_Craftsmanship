{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Libraries\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>sequence</th>\n",
       "      <th>pose_x11</th>\n",
       "      <th>pose_y11</th>\n",
       "      <th>pose_z11</th>\n",
       "      <th>pose_x12</th>\n",
       "      <th>pose_y12</th>\n",
       "      <th>pose_z12</th>\n",
       "      <th>pose_x13</th>\n",
       "      <th>...</th>\n",
       "      <th>left_hand_z17</th>\n",
       "      <th>left_hand_x18</th>\n",
       "      <th>left_hand_y18</th>\n",
       "      <th>left_hand_z18</th>\n",
       "      <th>left_hand_x19</th>\n",
       "      <th>left_hand_y19</th>\n",
       "      <th>left_hand_z19</th>\n",
       "      <th>left_hand_x20</th>\n",
       "      <th>left_hand_y20</th>\n",
       "      <th>left_hand_z20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pen</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.594059</td>\n",
       "      <td>0.309059</td>\n",
       "      <td>-0.196826</td>\n",
       "      <td>0.311200</td>\n",
       "      <td>0.339124</td>\n",
       "      <td>-0.057219</td>\n",
       "      <td>0.753011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034374</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>0.782419</td>\n",
       "      <td>-0.050452</td>\n",
       "      <td>0.351776</td>\n",
       "      <td>0.811713</td>\n",
       "      <td>-0.058446</td>\n",
       "      <td>0.355583</td>\n",
       "      <td>0.840286</td>\n",
       "      <td>-0.062885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pen</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.593844</td>\n",
       "      <td>0.309345</td>\n",
       "      <td>-0.196947</td>\n",
       "      <td>0.310788</td>\n",
       "      <td>0.339243</td>\n",
       "      <td>-0.057271</td>\n",
       "      <td>0.752674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pen</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.593748</td>\n",
       "      <td>0.309507</td>\n",
       "      <td>-0.193880</td>\n",
       "      <td>0.310502</td>\n",
       "      <td>0.339239</td>\n",
       "      <td>-0.066608</td>\n",
       "      <td>0.752274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041063</td>\n",
       "      <td>0.350450</td>\n",
       "      <td>0.785777</td>\n",
       "      <td>-0.057279</td>\n",
       "      <td>0.356252</td>\n",
       "      <td>0.815158</td>\n",
       "      <td>-0.064492</td>\n",
       "      <td>0.362689</td>\n",
       "      <td>0.842664</td>\n",
       "      <td>-0.068444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pen</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.593448</td>\n",
       "      <td>0.309622</td>\n",
       "      <td>-0.167050</td>\n",
       "      <td>0.309933</td>\n",
       "      <td>0.339824</td>\n",
       "      <td>-0.037587</td>\n",
       "      <td>0.751536</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051957</td>\n",
       "      <td>0.354071</td>\n",
       "      <td>0.783193</td>\n",
       "      <td>-0.066484</td>\n",
       "      <td>0.361161</td>\n",
       "      <td>0.810169</td>\n",
       "      <td>-0.070669</td>\n",
       "      <td>0.369577</td>\n",
       "      <td>0.833266</td>\n",
       "      <td>-0.072963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pen</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.593303</td>\n",
       "      <td>0.309749</td>\n",
       "      <td>-0.187792</td>\n",
       "      <td>0.309748</td>\n",
       "      <td>0.340211</td>\n",
       "      <td>-0.036565</td>\n",
       "      <td>0.750996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052415</td>\n",
       "      <td>0.352663</td>\n",
       "      <td>0.778692</td>\n",
       "      <td>-0.069039</td>\n",
       "      <td>0.359794</td>\n",
       "      <td>0.805952</td>\n",
       "      <td>-0.075388</td>\n",
       "      <td>0.368541</td>\n",
       "      <td>0.831612</td>\n",
       "      <td>-0.079067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  accuracy  sequence  pose_x11  pose_y11  pose_z11  pose_x12  pose_y12  \\\n",
       "0   pen         1         0  0.594059  0.309059 -0.196826  0.311200  0.339124   \n",
       "1   pen         1         0  0.593844  0.309345 -0.196947  0.310788  0.339243   \n",
       "2   pen         1         0  0.593748  0.309507 -0.193880  0.310502  0.339239   \n",
       "3   pen         1         0  0.593448  0.309622 -0.167050  0.309933  0.339824   \n",
       "4   pen         1         0  0.593303  0.309749 -0.187792  0.309748  0.340211   \n",
       "\n",
       "   pose_z12  pose_x13  ...  left_hand_z17  left_hand_x18  left_hand_y18  \\\n",
       "0 -0.057219  0.753011  ...      -0.034374       0.348000       0.782419   \n",
       "1 -0.057271  0.752674  ...       0.000000       0.000000       0.000000   \n",
       "2 -0.066608  0.752274  ...      -0.041063       0.350450       0.785777   \n",
       "3 -0.037587  0.751536  ...      -0.051957       0.354071       0.783193   \n",
       "4 -0.036565  0.750996  ...      -0.052415       0.352663       0.778692   \n",
       "\n",
       "   left_hand_z18  left_hand_x19  left_hand_y19  left_hand_z19  left_hand_x20  \\\n",
       "0      -0.050452       0.351776       0.811713      -0.058446       0.355583   \n",
       "1       0.000000       0.000000       0.000000       0.000000       0.000000   \n",
       "2      -0.057279       0.356252       0.815158      -0.064492       0.362689   \n",
       "3      -0.066484       0.361161       0.810169      -0.070669       0.369577   \n",
       "4      -0.069039       0.359794       0.805952      -0.075388       0.368541   \n",
       "\n",
       "   left_hand_y20  left_hand_z20  \n",
       "0       0.840286      -0.062885  \n",
       "1       0.000000       0.000000  \n",
       "2       0.842664      -0.068444  \n",
       "3       0.833266      -0.072963  \n",
       "4       0.831612      -0.079067  \n",
       "\n",
       "[5 rows x 165 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 9: Delete not important data\n",
    "data = pd.read_csv('combined_coordinates.csv')\n",
    "\n",
    "# Identify columns to remove: include specific ranges and all visibility points\n",
    "columns_to_remove_1 = [f\"pose_{c}{i}\" for c in ['x', 'y', 'z', 'v'] for i in range(0, 11)]\n",
    "columns_to_remove_2 = [f\"pose_{c}{i}\" for c in ['x', 'y', 'z', 'v'] for i in range(23, 33)]\n",
    "visibility_columns_pose = [col for col in data.columns if 'pose_v' in col]\n",
    "visibility_columns_left_hand = [col for col in data.columns if 'left_hand_v' in col]\n",
    "visibility_columns_right_hand = [col for col in data.columns if 'right_hand_v' in col]\n",
    "\n",
    "# Combine all columns to remove\n",
    "columns_to_remove = columns_to_remove_1 + columns_to_remove_2 + visibility_columns_pose + visibility_columns_left_hand + visibility_columns_right_hand\n",
    "columns_to_remove = [col for col in columns_to_remove if col in data.columns]\n",
    "\n",
    "# Drop the selected columns from the dataframe\n",
    "data_filtered = data.drop(columns=columns_to_remove)\n",
    "\n",
    "# Round the values and save the filtered data\n",
    "data_filtered.to_csv('filtered_coordinates.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the filtered dataset\n",
    "data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def transform_and_merge_columns(file_path, output_file_path):\n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    data['accuracy'] = data['accuracy'].map({0: 'W', 1: 'R'})\n",
    "    data['label'] = data.iloc[:, 0].astype(str) + '_' + data['accuracy']\n",
    "    \n",
    "    label_column = data.pop('label') \n",
    "    data.insert(0, 'label', label_column)  \n",
    "    \n",
    "    data.drop(columns=['class', 'accuracy'], inplace=True)\n",
    "    data.to_csv(output_file_path, index=False)\n",
    "\n",
    "input_file_path = 'filtered_coordinates.csv'  \n",
    "output_file_path = 'modified_file.csv'  \n",
    "transform_and_merge_columns(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill columns with 0s so you have the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paint_R', 'paint_W', 'pen_R', 'pen_W', 'scissors_R', 'scissors_W']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the contents of the text file\n",
    "with open('unique_labels.txt', 'r') as file:\n",
    "    actions = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Print the list of actions\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Your existing code to encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(actions)\n",
    "\n",
    "# Now you can use features_padded and labels_encoded for training or prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1715346486.807142  600678 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2 Pro\n",
      "I0000 00:00:1715346486.813250  600678 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2 Pro\n",
      "W0000 00:00:1715346486.823502  604132 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1715346486.828640  604130 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1715346486.907862  604120 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1715346486.917056  604118 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Check that the camera and mediapipe are working\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture\")\n",
    "else:\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose, \\\n",
    "        mp.solutions.hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "        while cap.isOpened():\n",
    "            ret, image = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Unable to read frame from video capture\")\n",
    "                break\n",
    "\n",
    "            # Flip image to simulate mirror view\n",
    "            image = cv2.flip(image, 1)\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Make detections\n",
    "            pose_results = pose.process(image)\n",
    "            hand_results = hands.process(image)\n",
    "\n",
    "            # RGB 2 BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Draw points\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2),\n",
    "                )\n",
    "\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image,\n",
    "                        hand_landmarks,\n",
    "                        mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                        mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                    )\n",
    "\n",
    "            cv2.imshow(\"Raw Webcam Feed\", image)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1715346688.281489  600678 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2 Pro\n",
      "I0000 00:00:1715346688.286270  600678 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2 Pro\n",
      "W0000 00:00:1715346688.291605  606890 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1715346688.295968  606890 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1715346688.376604  606871 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1715346688.384445  606871 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in all_landmarks: 156\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Number of elements in all_landmarks: 159\n",
      "Data shape: (10809,)\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Number of frames to collect data for\n",
    "num_frames_to_collect = 68\n",
    "\n",
    "# Initialize an array to store the collected data\n",
    "collected_data = []\n",
    "\n",
    "# Setup MediaPipe instances\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture\")\n",
    "else:\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose, \\\n",
    "         mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5, max_num_hands=2) as hands:\n",
    "        frame_count = 0\n",
    "        while cap.isOpened() and frame_count < num_frames_to_collect:\n",
    "            ret, image = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Unable to read frame from video capture\")\n",
    "                break\n",
    "\n",
    "            # Flip image to simulate mirror view\n",
    "            image = cv2.flip(image, 1)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Make detections\n",
    "            pose_results = pose.process(image)\n",
    "            hand_results = hands.process(image)\n",
    "\n",
    "            # Flatten pose landmarks, skipping certain indices\n",
    "            if pose_results.pose_landmarks:\n",
    "                pose_landmarks = [\n",
    "                    round(value, 3) for idx, landmark in enumerate(pose_results.pose_landmarks.landmark)\n",
    "                    if not (0 <= idx <= 11 or 23 <= idx <= 32)  # Adjust to 0-based indexing and include index 32\n",
    "                    for value in (landmark.x, landmark.y, landmark.z)\n",
    "                ]\n",
    "            else:\n",
    "                pose_landmarks = [0] * (10 * 3)  # Adjust count to reflect the remaining landmarks\n",
    "\n",
    "            # Initialize hand landmarks placeholders\n",
    "            right_hand_landmarks = [0] * (21 * 3)\n",
    "            left_hand_landmarks = [0] * (21 * 3)\n",
    "\n",
    "            # Detect and sort hand landmarks\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                hand_labels = [hand.classification[0].label for hand in hand_results.multi_handedness]\n",
    "                for hand_landmarks, label in zip(hand_results.multi_hand_landmarks, hand_labels):\n",
    "                    flat_hand = [round(value, 3) for landmark in hand_landmarks.landmark\n",
    "                                 for value in (landmark.x, landmark.y, landmark.z)]\n",
    "                    if label == 'Right':\n",
    "                        right_hand_landmarks = flat_hand\n",
    "                    else:\n",
    "                        left_hand_landmarks = flat_hand\n",
    "\n",
    "            # Combine all landmarks\n",
    "            all_landmarks = pose_landmarks + right_hand_landmarks + left_hand_landmarks\n",
    "\n",
    "            # Print the number of elements in all_landmarks for each frame\n",
    "            print(\"Number of elements in all_landmarks:\", len(all_landmarks))\n",
    "\n",
    "            collected_data.extend(all_landmarks)  # Append to flat list\n",
    "\n",
    "            # Drawing\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2),\n",
    "                )\n",
    "\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                        mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                    )\n",
    "\n",
    "            cv2.imshow(\"Raw Webcam Feed with Landmarks\", image)\n",
    "\n",
    "            # Increment frame count\n",
    "            frame_count += 1\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Convert to numpy array\n",
    "collected_data_array = np.array(collected_data)\n",
    "print(\"Data shape:\", collected_data_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of collected_data_array: 10809\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of collected_data_array:\", collected_data_array.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">58,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m58,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m198\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">212,776</span> (831.16 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m212,776\u001b[0m (831.16 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">212,774</span> (831.15 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m212,774\u001b[0m (831.15 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32,), dtype=float32). Expected shape (None, 68, 162), but input has incompatible shape (32,)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32,), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Assuming you have prepared your input data (features_padded) and encoded labels (labels_encoded)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# If not, prepare your data based on the input shape as mentioned before\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollected_data_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Print predictions\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictions)\n",
      "File \u001b[0;32m~/Desktop/ML_Craftsmanship/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/ML_Craftsmanship/tf/lib/python3.10/site-packages/keras/src/models/functional.py:288\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    286\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    287\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32,), dtype=float32). Expected shape (None, 68, 162), but input has incompatible shape (32,)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32,), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the .h5 model\n",
    "model = tf.keras.models.load_model('actions.h5')\n",
    "\n",
    "# Inspect the model's summary to see the input shape\n",
    "print(\"Model Summary:\")\n",
    "print(model.summary())\n",
    "\n",
    "# Assuming you have prepared your input data (features_padded) and encoded labels (labels_encoded)\n",
    "# If not, prepare your data based on the input shape as mentioned before\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(collected_data_array)\n",
    "\n",
    "# Print predictions\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "# If you want to decode the predictions back to original labels\n",
    "label_decoder = LabelEncoder()\n",
    "label_decoder.fit(actions)\n",
    "decoded_predictions = label_decoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "print(\"Decoded Predictions:\", decoded_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Create folders in directory\n",
    "data = pd.read_csv('filtered_coordinates.csv')\n",
    "\n",
    "base_dir = 'DataBase'\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "unique_classes = data['class'].unique()\n",
    "\n",
    "for class_value in unique_classes:\n",
    "    class_dir = os.path.join(base_dir, str(class_value))\n",
    "    if not os.path.exists(class_dir):\n",
    "        os.makedirs(class_dir)\n",
    "    \n",
    "    class_data = data[data['class'] == class_value]\n",
    "\n",
    "    for accuracy_type, subfolder_name in zip([1, 0], ['R', 'W']):\n",
    "        accuracy_dir = os.path.join(class_dir, subfolder_name)\n",
    "        if not os.path.exists(accuracy_dir):\n",
    "            os.makedirs(accuracy_dir)\n",
    "        \n",
    "        accuracy_data = class_data[class_data['accuracy'] == accuracy_type]\n",
    "        \n",
    "        unique_sequences = accuracy_data['sequence'].unique()\n",
    "        \n",
    "        for sequence in unique_sequences:\n",
    "            sequence_dir = os.path.join(accuracy_dir, str(sequence))\n",
    "            if not os.path.exists(sequence_dir):\n",
    "                os.makedirs(sequence_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Save individual np arrays\n",
    "data = pd.read_csv('filtered_coordinates.csv')\n",
    "base_dir = 'DataBase'\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "for class_value in data['class'].unique():\n",
    "    class_dir = os.path.join(base_dir, str(class_value))\n",
    "    class_data = data[data['class'] == class_value]\n",
    "\n",
    "    for accuracy_type, subfolder_name in zip([1, 0], ['R', 'W']):\n",
    "        accuracy_dir = os.path.join(class_dir, subfolder_name)\n",
    "        accuracy_data = class_data[class_data['accuracy'] == accuracy_type]\n",
    "\n",
    "        for sequence in accuracy_data['sequence'].unique():\n",
    "            sequence_dir = os.path.join(accuracy_dir, str(sequence))\n",
    "            sequence_data = accuracy_data[accuracy_data['sequence'] == sequence]\n",
    "\n",
    "            sequence_data = sequence_data.reset_index(drop=True)\n",
    "\n",
    "            for index, row in sequence_data.iterrows():\n",
    "                frame_path = os.path.join(sequence_dir, f'{index}.npy')\n",
    "                np.save(frame_path, row.values[3:])\n",
    "\n",
    "print(\"cosas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Test if array is in the correct way\n",
    "np.load('DataBase/pen/W/0/0.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
