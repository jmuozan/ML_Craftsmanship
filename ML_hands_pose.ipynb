{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install libraries\n",
    "!pip install mediapipe opencv-python pandas scikit-learn numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Libraries\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mediapipe initialization and Video recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 18:09:33.560 python[18370:620209] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1715357374.920824  620209 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1715357374.927667  620209 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2 Pro\n",
      "W0000 00:00:1715357374.934829  621609 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1715357374.940888  621609 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1715357375.023431  621600 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1715357375.031971  621597 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/jorgemuyo/Desktop/ML_Craftsmanship/tf/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Check that the camera and mediapipe are working\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture\")\n",
    "else:\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose, \\\n",
    "        mp.solutions.hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "        while cap.isOpened():\n",
    "            ret, image = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Unable to read frame from video capture\")\n",
    "                break\n",
    "\n",
    "            # Flip image to simulate mirror view\n",
    "            image = cv2.flip(image, 1)\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Make detections\n",
    "            pose_results = pose.process(image)\n",
    "            hand_results = hands.process(image)\n",
    "\n",
    "            # RGB 2 BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Draw points\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2),\n",
    "                )\n",
    "\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image,\n",
    "                        hand_landmarks,\n",
    "                        mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                        mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                    )\n",
    "\n",
    "            cv2.imshow(\"Raw Webcam Feed\", image)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Record video for data capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture\")\n",
    "else:\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30 \n",
    "    print(f\"Video Resolution: {width}x{height} at {fps} FPS\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID') \n",
    "    out = cv2.VideoWriter('output.avi', fourcc, fps, (width, height))\n",
    "\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Error: Unable to read frame from video capture\")\n",
    "            break\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = hands.process(frame_rgb)  \n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Write every frame\n",
    "        out.write(frame)\n",
    "\n",
    "        cv2.imshow('MediaPipe Hands', frame)  \n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'): \n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create CSV to store data\n",
    "headers = ['class', 'accuracy', 'sequence'] \n",
    "headers.extend([f'pose_{coord}{i}' for i in range(33) for coord in ('x', 'y', 'z', 'v')])\n",
    "headers.extend([f'{hand}_{coord}{i}' for hand in ('right_hand', 'left_hand') for i in range(21) for coord in ('x', 'y', 'z', 'v')])\n",
    "\n",
    "with open('coordinates_1.csv', mode='w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    csv_writer.writerow(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1715357486.405175  622479 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1715357486.410783  622479 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2 Pro\n",
      "W0000 00:00:1715357486.416817  622888 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1715357486.423298  622888 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1715357486.505413  622869 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1715357486.513433  622870 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/jorgemuyo/Desktop/ML_Craftsmanship/tf/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Capture data for first movement\n",
    "cap = cv2.VideoCapture('output_paint.avi')\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture\")\n",
    "else:\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('processed_output.avi', fourcc, fps, (width, height))\n",
    "\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    pose_drawing_spec = mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4)\n",
    "    hand_drawing_spec = mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "\n",
    "    record = False\n",
    "    accuracy = None\n",
    "    sequences = {'r': -1, 'w': -1}  \n",
    "    recording_state = None\n",
    "\n",
    "    with open('coordinates_1.csv', mode='a', newline='') as file:\n",
    "        csv_writer = csv.writer(file)\n",
    "\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Error: Unable to read frame from video capture\")\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_rgb.flags.writeable = False\n",
    "            pose_results = pose.process(frame_rgb)\n",
    "            hand_results = hands.process(frame_rgb)\n",
    "            frame_rgb.flags.writeable = True\n",
    "            frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=pose_drawing_spec,\n",
    "                    connection_drawing_spec=pose_drawing_spec)\n",
    "\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        frame,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        landmark_drawing_spec=hand_drawing_spec,\n",
    "                        connection_drawing_spec=hand_drawing_spec)\n",
    "\n",
    "            # Select the sequence number based on the key pressed\n",
    "            current_sequence = sequences[recording_state] if recording_state else -1\n",
    "            row = ['', accuracy, current_sequence]\n",
    "\n",
    "            if pose_results.pose_landmarks:\n",
    "                for lm in pose_results.pose_landmarks.landmark:\n",
    "                    visibility_binary = 1 if lm.visibility > 0.3 else 0\n",
    "                    row.extend([lm.x, lm.y, lm.z, visibility_binary])\n",
    "            else:\n",
    "                row.extend([0] * 33 * 4)\n",
    "\n",
    "            for hand in ('right_hand', 'left_hand'):\n",
    "                found = False\n",
    "                if hand_results.multi_hand_landmarks:\n",
    "                    for hand_landmarks, handedness in zip(hand_results.multi_hand_landmarks, hand_results.multi_handedness):\n",
    "                        if handedness.classification[0].label == ('Right' if hand == 'right_hand' else 'Left'):\n",
    "                            for lm in hand_landmarks.landmark:\n",
    "                                visibility_binary = 1 if lm.visibility > 0.2 else 0\n",
    "                                row.extend([lm.x, lm.y, lm.z, visibility_binary])\n",
    "                            found = True\n",
    "                            break\n",
    "                if not found:\n",
    "                    row.extend([0] * 21 * 4)\n",
    "\n",
    "            if record:\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "            out.write(frame)\n",
    "            cv2.imshow('MediaPipe Pose', frame)\n",
    "            key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "            if key == ord('r') or key == ord('w'):\n",
    "                new_state = chr(key)\n",
    "                if new_state != recording_state:\n",
    "                    sequences[new_state] += 1  \n",
    "                    recording_state = new_state\n",
    "                record = True\n",
    "                accuracy = 1 if key == ord('r') else 0\n",
    "            elif key == ord('s'):\n",
    "                record = False\n",
    "                recording_state = None\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "df = pd.read_csv('coordinates_1.csv')\n",
    "df.loc[df['accuracy'].notna(), 'class'] = 'pen'\n",
    "df.to_csv('coordinates_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Capture data for second movement\n",
    "cap = cv2.VideoCapture('output_paint.avi')\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture\")\n",
    "else:\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('processed_output.avi', fourcc, fps, (width, height))\n",
    "\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    pose_drawing_spec = mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4)\n",
    "    hand_drawing_spec = mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "\n",
    "    record = False\n",
    "    accuracy = None\n",
    "    sequences = {'r': -1, 'w': -1}  \n",
    "    recording_state = None\n",
    "\n",
    "    with open('coordinates_2.csv', mode='a', newline='') as file:\n",
    "        csv_writer = csv.writer(file, lineterminator='\\n')\n",
    "\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Error: Unable to read frame from video capture\")\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_rgb.flags.writeable = False\n",
    "            pose_results = pose.process(frame_rgb)\n",
    "            hand_results = hands.process(frame_rgb)\n",
    "            frame_rgb.flags.writeable = True\n",
    "            frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=pose_drawing_spec,\n",
    "                    connection_drawing_spec=pose_drawing_spec)\n",
    "\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        frame,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        landmark_drawing_spec=hand_drawing_spec,\n",
    "                        connection_drawing_spec=hand_drawing_spec)\n",
    "\n",
    "            current_sequence = sequences[recording_state] if recording_state else -1\n",
    "            row = ['', accuracy, current_sequence]\n",
    "\n",
    "            if pose_results.pose_landmarks:\n",
    "                for lm in pose_results.pose_landmarks.landmark:\n",
    "                    visibility_binary = 1 if lm.visibility > 0.3 else 0\n",
    "                    row.extend([lm.x, lm.y, lm.z, visibility_binary])\n",
    "            else:\n",
    "                row.extend([0] * 33 * 4)\n",
    "\n",
    "            for hand in ('right_hand', 'left_hand'):\n",
    "                found = False\n",
    "                if hand_results.multi_hand_landmarks:\n",
    "                    for hand_landmarks, handedness in zip(hand_results.multi_hand_landmarks, hand_results.multi_handedness):\n",
    "                        if handedness.classification[0].label == ('Right' if hand == 'right_hand' else 'Left'):\n",
    "                            for lm in hand_landmarks.landmark:\n",
    "                                visibility_binary = 1 if lm.visibility > 0.2 else 0\n",
    "                                row.extend([lm.x, lm.y, lm.z, visibility_binary])\n",
    "                            found = True\n",
    "                            break\n",
    "                if not found:\n",
    "                    row.extend([0] * 21 * 4)\n",
    "\n",
    "            if record:\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "            out.write(frame)\n",
    "            cv2.imshow('MediaPipe Pose', frame)\n",
    "            key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "            if key == ord('r') or key == ord('w'):\n",
    "                new_state = chr(key)\n",
    "                if new_state != recording_state:\n",
    "                    sequences[new_state] += 1  \n",
    "                    recording_state = new_state\n",
    "                record = True\n",
    "                accuracy = 1 if key == ord('r') else 0\n",
    "            elif key == ord('s'):\n",
    "                record = False\n",
    "                recording_state = None\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "df = pd.read_csv('coordinates_2.csv', header=None)\n",
    "df.loc[df[1].notna(), 0] = 'paint'\n",
    "df.to_csv('coordinates_2.csv', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.2: Capture data for third movement\n",
    "cap = cv2.VideoCapture('output_scissors.avi')\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture\")\n",
    "else:\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('processed_output.avi', fourcc, fps, (width, height))\n",
    "\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    pose_drawing_spec = mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4)\n",
    "    hand_drawing_spec = mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "\n",
    "    record = False\n",
    "    accuracy = None\n",
    "    sequences = {'r': -1, 'w': -1}  \n",
    "    recording_state = None\n",
    "\n",
    "    with open('coordinates_3.csv', mode='a', newline='') as file:\n",
    "        csv_writer = csv.writer(file, lineterminator='\\n')\n",
    "\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Error: Unable to read frame from video capture\")\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_rgb.flags.writeable = False\n",
    "            pose_results = pose.process(frame_rgb)\n",
    "            hand_results = hands.process(frame_rgb)\n",
    "            frame_rgb.flags.writeable = True\n",
    "            frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=pose_drawing_spec,\n",
    "                    connection_drawing_spec=pose_drawing_spec)\n",
    "\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        frame,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        landmark_drawing_spec=hand_drawing_spec,\n",
    "                        connection_drawing_spec=hand_drawing_spec)\n",
    "\n",
    "            current_sequence = sequences[recording_state] if recording_state else -1\n",
    "            row = ['', accuracy, current_sequence]\n",
    "\n",
    "            if pose_results.pose_landmarks:\n",
    "                for lm in pose_results.pose_landmarks.landmark:\n",
    "                    visibility_binary = 1 if lm.visibility > 0.3 else 0\n",
    "                    row.extend([lm.x, lm.y, lm.z, visibility_binary])\n",
    "            else:\n",
    "                row.extend([0] * 33 * 4)\n",
    "\n",
    "            for hand in ('right_hand', 'left_hand'):\n",
    "                found = False\n",
    "                if hand_results.multi_hand_landmarks:\n",
    "                    for hand_landmarks, handedness in zip(hand_results.multi_hand_landmarks, hand_results.multi_handedness):\n",
    "                        if handedness.classification[0].label == ('Right' if hand == 'right_hand' else 'Left'):\n",
    "                            for lm in hand_landmarks.landmark:\n",
    "                                visibility_binary = 1 if lm.visibility > 0.2 else 0\n",
    "                                row.extend([lm.x, lm.y, lm.z, visibility_binary])\n",
    "                            found = True\n",
    "                            break\n",
    "                if not found:\n",
    "                    row.extend([0] * 21 * 4)\n",
    "\n",
    "            if record:\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "            out.write(frame)\n",
    "            cv2.imshow('MediaPipe Pose', frame)\n",
    "            key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "            if key == ord('r') or key == ord('w'):\n",
    "                new_state = chr(key)\n",
    "                if new_state != recording_state:\n",
    "                    sequences[new_state] += 1  \n",
    "                    recording_state = new_state\n",
    "                record = True\n",
    "                accuracy = 1 if key == ord('r') else 0\n",
    "            elif key == ord('s'):\n",
    "                record = False\n",
    "                recording_state = None\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "df = pd.read_csv('coordinates_3.csv', header=None)\n",
    "df.loc[df[1].notna(), 0] = 'scissors'\n",
    "df.to_csv('coordinates_3.csv', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Join all the CSVs of the different movements\n",
    "df1 = pd.read_csv('coordinates_1.csv', header=None)\n",
    "df2 = pd.read_csv('coordinates_2.csv', header=None)\n",
    "df3 = pd.read_csv('coordinates_3.csv', header=None)\n",
    "\n",
    "\n",
    "combined_df = pd.concat([df1, df2, df3], axis=0, ignore_index=True)\n",
    "combined_path = 'combined_coordinates.csv'\n",
    "combined_df.to_csv(combined_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Delete not important data\n",
    "data = pd.read_csv('combined_coordinates.csv')\n",
    "\n",
    "# Identify columns to remove: include specific ranges and all visibility points\n",
    "columns_to_remove_1 = [f\"pose_{c}{i}\" for c in ['x', 'y', 'z', 'v'] for i in range(0, 11)]\n",
    "columns_to_remove_2 = [f\"pose_{c}{i}\" for c in ['x', 'y', 'z', 'v'] for i in range(23, 33)]\n",
    "visibility_columns_pose = [col for col in data.columns if 'pose_v' in col]\n",
    "visibility_columns_left_hand = [col for col in data.columns if 'left_hand_v' in col]\n",
    "visibility_columns_right_hand = [col for col in data.columns if 'right_hand_v' in col]\n",
    "\n",
    "# Combine all columns to remove\n",
    "columns_to_remove = columns_to_remove_1 + columns_to_remove_2 + visibility_columns_pose + visibility_columns_left_hand + visibility_columns_right_hand\n",
    "columns_to_remove = [col for col in columns_to_remove if col in data.columns]\n",
    "\n",
    "# Drop the selected columns from the dataframe\n",
    "data_filtered = data.drop(columns=columns_to_remove)\n",
    "\n",
    "# Round the values and save the filtered data\n",
    "data_filtered = data_filtered.round(3)\n",
    "data_filtered.to_csv('filtered_coordinates.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the filtered dataset\n",
    "data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Create folders in directory\n",
    "data = pd.read_csv('filtered_coordinates.csv')\n",
    "\n",
    "base_dir = 'DataBase'\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "unique_classes = data['class'].unique()\n",
    "\n",
    "for class_value in unique_classes:\n",
    "    class_dir = os.path.join(base_dir, str(class_value))\n",
    "    if not os.path.exists(class_dir):\n",
    "        os.makedirs(class_dir)\n",
    "    \n",
    "    class_data = data[data['class'] == class_value]\n",
    "\n",
    "    for accuracy_type, subfolder_name in zip([1, 0], ['R', 'W']):\n",
    "        accuracy_dir = os.path.join(class_dir, subfolder_name)\n",
    "        if not os.path.exists(accuracy_dir):\n",
    "            os.makedirs(accuracy_dir)\n",
    "        \n",
    "        accuracy_data = class_data[class_data['accuracy'] == accuracy_type]\n",
    "        \n",
    "        unique_sequences = accuracy_data['sequence'].unique()\n",
    "        \n",
    "        for sequence in unique_sequences:\n",
    "            sequence_dir = os.path.join(accuracy_dir, str(sequence))\n",
    "            if not os.path.exists(sequence_dir):\n",
    "                os.makedirs(sequence_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Save individual np arrays\n",
    "data = pd.read_csv('filtered_coordinates.csv')\n",
    "base_dir = 'DataBase'\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "for class_value in data['class'].unique():\n",
    "    class_dir = os.path.join(base_dir, str(class_value))\n",
    "    class_data = data[data['class'] == class_value]\n",
    "\n",
    "    for accuracy_type, subfolder_name in zip([1, 0], ['R', 'W']):\n",
    "        accuracy_dir = os.path.join(class_dir, subfolder_name)\n",
    "        accuracy_data = class_data[class_data['accuracy'] == accuracy_type]\n",
    "\n",
    "        for sequence in accuracy_data['sequence'].unique():\n",
    "            sequence_dir = os.path.join(accuracy_dir, str(sequence))\n",
    "            sequence_data = accuracy_data[accuracy_data['sequence'] == sequence]\n",
    "\n",
    "            sequence_data = sequence_data.reset_index(drop=True)\n",
    "\n",
    "            for index, row in sequence_data.iterrows():\n",
    "                frame_path = os.path.join(sequence_dir, f'{index}.npy')\n",
    "                np.save(frame_path, row.values[3:])\n",
    "\n",
    "print(\"cosas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Test if array is in the correct way\n",
    "np.load('DataBase/scissors/W/4/0.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array=np.load('DataBase/movement_1/W/0/0.npy', allow_pickle=True)\n",
    "len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Install new libraries for processing and training\n",
    "!pip install tensorflow \n",
    "!pip install tensorflow-macos\n",
    "!pip install tensorflow-metal\n",
    "!pip install numpy --upgrade\n",
    "!pip install pandas --upgrade\n",
    "!pip install matplotlib --upgrade\n",
    "!pip install scikit-learn --upgrade\n",
    "!pip install scipy --upgrade\n",
    "!pip install plotly --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y apple tensorflow-deps\n",
    "!conda install notebook -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "\n",
    "print (f\"Python Platform: {platform.platform ()}\")\n",
    "print (f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print ()\n",
    "\n",
    "print (f\"Python {sys.version}\")\n",
    "print (f\"Pandas {pd.__version__}\")\n",
    "print (f\"Scikit-Learn {sk.__version__}\")\n",
    "print (f\"SciPy {sp.__version__}\")\n",
    "gpu = len (tf.config.list_physical_devices ('GPU'))>0\n",
    "print (\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Import new libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Joining and labeling arrays\n",
    "base_dir = 'DataBase'\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for class_name in os.listdir(base_dir):\n",
    "    class_dir = os.path.join(base_dir, class_name)\n",
    "    for accuracy_type in ['R', 'W']:\n",
    "        accuracy_dir = os.path.join(class_dir, accuracy_type)\n",
    "        for sequence in os.listdir(accuracy_dir):\n",
    "            sequence_dir = os.path.join(accuracy_dir, sequence)\n",
    "            sequence_features = []\n",
    "            for file_name in os.listdir(sequence_dir):\n",
    "                file_path = os.path.join(sequence_dir, file_name)\n",
    "                data = np.load(file_path, allow_pickle=True)\n",
    "                sequence_features.append(data)\n",
    "            features.append(sequence_features)\n",
    "            labels.append(f\"{class_name}_{accuracy_type}\")\n",
    "\n",
    "features_padded = pad_sequences(features, padding='post', dtype='float32')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell after 15: Extract and print unique labels before encoding\n",
    "# This code assumes 'labels' list is already populated as in Cell 15\n",
    "\n",
    "# Extract unique labels\n",
    "unique_labels = set(labels)\n",
    "\n",
    "# Print unique labels\n",
    "print(\"Unique labels before encoding:\")\n",
    "print(unique_labels)\n",
    "\n",
    "# Optionally, if you want to save these unique labels to a text file:\n",
    "with open('unique_labels.txt', 'w') as f:\n",
    "    for label in sorted(unique_labels):  # Sorting to maintain consistent order\n",
    "        f.write(\"%s\\n\" % label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Labels test (features_padded and labels_encoded) outputs expected in .shape f_p(x, y, z) / l_e(x, )\n",
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Set X and y\n",
    "X = np.array(features_padded)\n",
    "y = to_categorical(labels_encoded).astype(int)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume X is your existing NumPy array\n",
    "# Example: X = np.random.rand(10, 10, 10) - Replace with your actual array\n",
    "\n",
    "# Set numpy print options to avoid truncation\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# Convert the array X to string\n",
    "array_str = np.array2string(X)\n",
    "\n",
    "# Write the string to a file\n",
    "with open('X.txt', 'w') as f:\n",
    "    f.write(array_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y.txt', 'w') as file:\n",
    "    # Convert the list into a string and write it to the file\n",
    "    file.write(str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Split values for testing and training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model + LSTM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Import training dependencies\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: To web monitor live training later\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6  # Put here amount of movements you want to detect\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(68, 162)))  # Put here las two values of 'X.shape' in this case X.shape = (122, 68, 162)\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='tanh')) \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [0.7, 0.3, 0.5, 0.4, 0.3, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_padded[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep track of the training in your terminal cd to Logs/train folder and then write tensorboard --logdir=. a link will pop up that you just need to put in your search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have a single set of features to predict\n",
    "model_output = model.predict(np.expand_dims(features_padded[100], axis=0))  \n",
    "predicted_index = np.argmax(model_output[0])  \n",
    "predicted_label = label_encoder.inverse_transform([predicted_index])[0]  \n",
    "\n",
    "print(\"Predicted Label:\", predicted_label)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('actions.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()  \n",
    "yhat = np.argmax(yhat, axis=1).tolist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real time predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the contents of the text file\n",
    "with open('unique_labels.txt', 'r') as file:\n",
    "    actions = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Print the list of actions\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Number of frames to collect data for\n",
    "num_frames_to_collect = 68\n",
    "\n",
    "# Initialize an array to store the collected data\n",
    "collected_data = []\n",
    "\n",
    "# Setup MediaPipe instances\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture\")\n",
    "else:\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose, \\\n",
    "         mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5, max_num_hands=2) as hands:\n",
    "        frame_count = 0\n",
    "        while cap.isOpened() and frame_count < num_frames_to_collect:\n",
    "            ret, image = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Unable to read frame from video capture\")\n",
    "                break\n",
    "\n",
    "            # Flip image to simulate mirror view\n",
    "            image = cv2.flip(image, 1)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Make detections\n",
    "            pose_results = pose.process(image)\n",
    "            hand_results = hands.process(image)\n",
    "\n",
    "            # Flatten pose landmarks, skipping certain indices\n",
    "            if pose_results.pose_landmarks:\n",
    "                pose_landmarks = [\n",
    "                    round(value, 3) for idx, landmark in enumerate(pose_results.pose_landmarks.landmark)\n",
    "                    if not (0 <= idx <= 11 or 23 <= idx <= 32)  # Adjust to 0-based indexing and include index 32\n",
    "                    for value in (landmark.x, landmark.y, landmark.z)\n",
    "                ]\n",
    "            else:\n",
    "                pose_landmarks = [0] * (10 * 3)  # Adjust count to reflect the remaining landmarks\n",
    "\n",
    "            # Initialize hand landmarks placeholders\n",
    "            right_hand_landmarks = [0] * (21 * 3)\n",
    "            left_hand_landmarks = [0] * (21 * 3)\n",
    "\n",
    "            # Detect and sort hand landmarks\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                hand_labels = [hand.classification[0].label for hand in hand_results.multi_handedness]\n",
    "                for hand_landmarks, label in zip(hand_results.multi_hand_landmarks, hand_labels):\n",
    "                    flat_hand = [round(value, 3) for landmark in hand_landmarks.landmark\n",
    "                                 for value in (landmark.x, landmark.y, landmark.z)]\n",
    "                    if label == 'Right':\n",
    "                        right_hand_landmarks = flat_hand\n",
    "                    else:\n",
    "                        left_hand_landmarks = flat_hand\n",
    "\n",
    "            # Combine all landmarks\n",
    "            all_landmarks = pose_landmarks + right_hand_landmarks + left_hand_landmarks\n",
    "            collected_data.extend(all_landmarks)  # Append to flat list\n",
    "\n",
    "            # Drawing\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2),\n",
    "                )\n",
    "\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                        mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                    )\n",
    "\n",
    "            cv2.imshow(\"Raw Webcam Feed with Landmarks\", image)\n",
    "\n",
    "            # Increment frame count\n",
    "            frame_count += 1\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Convert to numpy array\n",
    "collected_data_array = np.array(collected_data)\n",
    "print(\"Data shape:\", collected_data_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put time to wait\n",
    "# Import model\n",
    "# make predictions straight to matrix \n",
    "# Convert matrix results into labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your model\n",
    "model = load_model('actions.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your model\n",
    "model = load_model('actions.h5')\n",
    "\n",
    "# Example data for one sequence\n",
    "# Assume 'sequence' is a list of arrays, with each array representing frame data which is then flattened\n",
    "sequence = collected_data_array  # Replace these with actual frame data\n",
    "\n",
    "# If your model expects a 2D array (batch size, features):\n",
    "sequence = np.expand_dims(sequence, axis=0)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(sequence)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynewv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
