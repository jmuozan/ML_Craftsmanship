{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in ./tf/lib/python3.10/site-packages (0.10.14)\n",
      "Requirement already satisfied: opencv-python in ./tf/lib/python3.10/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: pandas in ./tf/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in ./tf/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in ./tf/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in ./tf/lib/python3.10/site-packages (3.8.4)\n",
      "Requirement already satisfied: absl-py in ./tf/lib/python3.10/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in ./tf/lib/python3.10/site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in ./tf/lib/python3.10/site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in ./tf/lib/python3.10/site-packages (from mediapipe) (0.4.27)\n",
      "Requirement already satisfied: jaxlib in ./tf/lib/python3.10/site-packages (from mediapipe) (0.4.27)\n",
      "Requirement already satisfied: opencv-contrib-python in ./tf/lib/python3.10/site-packages (from mediapipe) (4.9.0.80)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in ./tf/lib/python3.10/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in ./tf/lib/python3.10/site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./tf/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./tf/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./tf/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./tf/lib/python3.10/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./tf/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./tf/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./tf/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./tf/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./tf/lib/python3.10/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./tf/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./tf/lib/python3.10/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in ./tf/lib/python3.10/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./tf/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in ./tf/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in ./tf/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in ./tf/lib/python3.10/site-packages (from jax->mediapipe) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum in ./tf/lib/python3.10/site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: pycparser in ./tf/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install libraries\n",
    "!pip install mediapipe opencv-python pandas scikit-learn numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Libraries\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mediapipe initialization and Video recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Check that the camera and mediapipe are working\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture\")\n",
    "else:\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose, \\\n",
    "        mp.solutions.hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "        while cap.isOpened():\n",
    "            ret, image = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Unable to read frame from video capture\")\n",
    "                break\n",
    "\n",
    "            # Flip image to simulate mirror view\n",
    "            image = cv2.flip(image, 1)\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Make detections\n",
    "            pose_results = pose.process(image)\n",
    "            hand_results = hands.process(image)\n",
    "\n",
    "            # RGB 2 BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Draw points\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2),\n",
    "                )\n",
    "\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image,\n",
    "                        hand_landmarks,\n",
    "                        mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                        mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                    )\n",
    "\n",
    "            cv2.imshow(\"Raw Webcam Feed\", image)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Record video for data capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture\")\n",
    "else:\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30 \n",
    "    print(f\"Video Resolution: {width}x{height} at {fps} FPS\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID') \n",
    "    out = cv2.VideoWriter('output.avi', fourcc, fps, (width, height))\n",
    "\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Error: Unable to read frame from video capture\")\n",
    "            break\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = hands.process(frame_rgb)  \n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Write every frame\n",
    "        out.write(frame)\n",
    "\n",
    "        cv2.imshow('MediaPipe Hands', frame)  \n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'): \n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create CSV to store data\n",
    "headers = ['class', 'accuracy', 'sequence'] \n",
    "headers.extend([f'pose_{coord}{i}' for i in range(33) for coord in ('x', 'y', 'z', 'v')])\n",
    "headers.extend([f'{hand}_{coord}{i}' for hand in ('right_hand', 'left_hand') for i in range(21) for coord in ('x', 'y', 'z', 'v')])\n",
    "\n",
    "with open('coordinates_1.csv', mode='w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    csv_writer.writerow(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Capture data for first movement\n",
    "cap = cv2.VideoCapture('output_paint.avi')\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture\")\n",
    "else:\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('processed_output.avi', fourcc, fps, (width, height))\n",
    "\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    pose_drawing_spec = mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4)\n",
    "    hand_drawing_spec = mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "\n",
    "    record = False\n",
    "    accuracy = None\n",
    "    sequences = {'r': -1, 'w': -1}  \n",
    "    recording_state = None\n",
    "\n",
    "    with open('coordinates_1.csv', mode='a', newline='') as file:\n",
    "        csv_writer = csv.writer(file)\n",
    "\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Error: Unable to read frame from video capture\")\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_rgb.flags.writeable = False\n",
    "            pose_results = pose.process(frame_rgb)\n",
    "            hand_results = hands.process(frame_rgb)\n",
    "            frame_rgb.flags.writeable = True\n",
    "            frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=pose_drawing_spec,\n",
    "                    connection_drawing_spec=pose_drawing_spec)\n",
    "\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        frame,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        landmark_drawing_spec=hand_drawing_spec,\n",
    "                        connection_drawing_spec=hand_drawing_spec)\n",
    "\n",
    "            # Select the sequence number based on the key pressed\n",
    "            current_sequence = sequences[recording_state] if recording_state else -1\n",
    "            row = ['', accuracy, current_sequence]\n",
    "\n",
    "            if pose_results.pose_landmarks:\n",
    "                for lm in pose_results.pose_landmarks.landmark:\n",
    "                    visibility_binary = 1 if lm.visibility > 0.3 else 0\n",
    "                    row.extend([lm.x, lm.y, lm.z, visibility_binary])\n",
    "            else:\n",
    "                row.extend([0] * 33 * 4)\n",
    "\n",
    "            for hand in ('right_hand', 'left_hand'):\n",
    "                found = False\n",
    "                if hand_results.multi_hand_landmarks:\n",
    "                    for hand_landmarks, handedness in zip(hand_results.multi_hand_landmarks, hand_results.multi_handedness):\n",
    "                        if handedness.classification[0].label == ('Right' if hand == 'right_hand' else 'Left'):\n",
    "                            for lm in hand_landmarks.landmark:\n",
    "                                visibility_binary = 1 if lm.visibility > 0.2 else 0\n",
    "                                row.extend([lm.x, lm.y, lm.z, visibility_binary])\n",
    "                            found = True\n",
    "                            break\n",
    "                if not found:\n",
    "                    row.extend([0] * 21 * 4)\n",
    "\n",
    "            if record:\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "            out.write(frame)\n",
    "            cv2.imshow('MediaPipe Pose', frame)\n",
    "            key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "            if key == ord('r') or key == ord('w'):\n",
    "                new_state = chr(key)\n",
    "                if new_state != recording_state:\n",
    "                    sequences[new_state] += 1  \n",
    "                    recording_state = new_state\n",
    "                record = True\n",
    "                accuracy = 1 if key == ord('r') else 0\n",
    "            elif key == ord('s'):\n",
    "                record = False\n",
    "                recording_state = None\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "df = pd.read_csv('coordinates_1.csv')\n",
    "df.loc[df['accuracy'].notna(), 'class'] = 'pen'\n",
    "df.to_csv('coordinates_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Capture data for second movement\n",
    "cap = cv2.VideoCapture('output_paint.avi')\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture\")\n",
    "else:\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('processed_output.avi', fourcc, fps, (width, height))\n",
    "\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    pose_drawing_spec = mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4)\n",
    "    hand_drawing_spec = mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "\n",
    "    record = False\n",
    "    accuracy = None\n",
    "    sequences = {'r': -1, 'w': -1}  \n",
    "    recording_state = None\n",
    "\n",
    "    with open('coordinates_2.csv', mode='a', newline='') as file:\n",
    "        csv_writer = csv.writer(file, lineterminator='\\n')\n",
    "\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Error: Unable to read frame from video capture\")\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_rgb.flags.writeable = False\n",
    "            pose_results = pose.process(frame_rgb)\n",
    "            hand_results = hands.process(frame_rgb)\n",
    "            frame_rgb.flags.writeable = True\n",
    "            frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=pose_drawing_spec,\n",
    "                    connection_drawing_spec=pose_drawing_spec)\n",
    "\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        frame,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        landmark_drawing_spec=hand_drawing_spec,\n",
    "                        connection_drawing_spec=hand_drawing_spec)\n",
    "\n",
    "            current_sequence = sequences[recording_state] if recording_state else -1\n",
    "            row = ['', accuracy, current_sequence]\n",
    "\n",
    "            if pose_results.pose_landmarks:\n",
    "                for lm in pose_results.pose_landmarks.landmark:\n",
    "                    visibility_binary = 1 if lm.visibility > 0.3 else 0\n",
    "                    row.extend([lm.x, lm.y, lm.z, visibility_binary])\n",
    "            else:\n",
    "                row.extend([0] * 33 * 4)\n",
    "\n",
    "            for hand in ('right_hand', 'left_hand'):\n",
    "                found = False\n",
    "                if hand_results.multi_hand_landmarks:\n",
    "                    for hand_landmarks, handedness in zip(hand_results.multi_hand_landmarks, hand_results.multi_handedness):\n",
    "                        if handedness.classification[0].label == ('Right' if hand == 'right_hand' else 'Left'):\n",
    "                            for lm in hand_landmarks.landmark:\n",
    "                                visibility_binary = 1 if lm.visibility > 0.2 else 0\n",
    "                                row.extend([lm.x, lm.y, lm.z, visibility_binary])\n",
    "                            found = True\n",
    "                            break\n",
    "                if not found:\n",
    "                    row.extend([0] * 21 * 4)\n",
    "\n",
    "            if record:\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "            out.write(frame)\n",
    "            cv2.imshow('MediaPipe Pose', frame)\n",
    "            key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "            if key == ord('r') or key == ord('w'):\n",
    "                new_state = chr(key)\n",
    "                if new_state != recording_state:\n",
    "                    sequences[new_state] += 1  \n",
    "                    recording_state = new_state\n",
    "                record = True\n",
    "                accuracy = 1 if key == ord('r') else 0\n",
    "            elif key == ord('s'):\n",
    "                record = False\n",
    "                recording_state = None\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "df = pd.read_csv('coordinates_2.csv', header=None)\n",
    "df.loc[df[1].notna(), 0] = 'paint'\n",
    "df.to_csv('coordinates_2.csv', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.2: Capture data for third movement\n",
    "cap = cv2.VideoCapture('output_scissors.avi')\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture\")\n",
    "else:\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('processed_output.avi', fourcc, fps, (width, height))\n",
    "\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    pose_drawing_spec = mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4)\n",
    "    hand_drawing_spec = mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "\n",
    "    record = False\n",
    "    accuracy = None\n",
    "    sequences = {'r': -1, 'w': -1}  \n",
    "    recording_state = None\n",
    "\n",
    "    with open('coordinates_3.csv', mode='a', newline='') as file:\n",
    "        csv_writer = csv.writer(file, lineterminator='\\n')\n",
    "\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Error: Unable to read frame from video capture\")\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_rgb.flags.writeable = False\n",
    "            pose_results = pose.process(frame_rgb)\n",
    "            hand_results = hands.process(frame_rgb)\n",
    "            frame_rgb.flags.writeable = True\n",
    "            frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=pose_drawing_spec,\n",
    "                    connection_drawing_spec=pose_drawing_spec)\n",
    "\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        frame,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        landmark_drawing_spec=hand_drawing_spec,\n",
    "                        connection_drawing_spec=hand_drawing_spec)\n",
    "\n",
    "            current_sequence = sequences[recording_state] if recording_state else -1\n",
    "            row = ['', accuracy, current_sequence]\n",
    "\n",
    "            if pose_results.pose_landmarks:\n",
    "                for lm in pose_results.pose_landmarks.landmark:\n",
    "                    visibility_binary = 1 if lm.visibility > 0.3 else 0\n",
    "                    row.extend([lm.x, lm.y, lm.z, visibility_binary])\n",
    "            else:\n",
    "                row.extend([0] * 33 * 4)\n",
    "\n",
    "            for hand in ('right_hand', 'left_hand'):\n",
    "                found = False\n",
    "                if hand_results.multi_hand_landmarks:\n",
    "                    for hand_landmarks, handedness in zip(hand_results.multi_hand_landmarks, hand_results.multi_handedness):\n",
    "                        if handedness.classification[0].label == ('Right' if hand == 'right_hand' else 'Left'):\n",
    "                            for lm in hand_landmarks.landmark:\n",
    "                                visibility_binary = 1 if lm.visibility > 0.2 else 0\n",
    "                                row.extend([lm.x, lm.y, lm.z, visibility_binary])\n",
    "                            found = True\n",
    "                            break\n",
    "                if not found:\n",
    "                    row.extend([0] * 21 * 4)\n",
    "\n",
    "            if record:\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "            out.write(frame)\n",
    "            cv2.imshow('MediaPipe Pose', frame)\n",
    "            key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "            if key == ord('r') or key == ord('w'):\n",
    "                new_state = chr(key)\n",
    "                if new_state != recording_state:\n",
    "                    sequences[new_state] += 1  \n",
    "                    recording_state = new_state\n",
    "                record = True\n",
    "                accuracy = 1 if key == ord('r') else 0\n",
    "            elif key == ord('s'):\n",
    "                record = False\n",
    "                recording_state = None\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "df = pd.read_csv('coordinates_3.csv', header=None)\n",
    "df.loc[df[1].notna(), 0] = 'scissors'\n",
    "df.to_csv('coordinates_3.csv', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Join all the CSVs of the different movements\n",
    "df1 = pd.read_csv('coordinates_1.csv', header=None)\n",
    "df2 = pd.read_csv('coordinates_2.csv', header=None)\n",
    "df3 = pd.read_csv('coordinates_3.csv', header=None)\n",
    "\n",
    "\n",
    "combined_df = pd.concat([df1, df2, df3], axis=0, ignore_index=True)\n",
    "combined_path = 'combined_coordinates.csv'\n",
    "combined_df.to_csv(combined_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>sequence</th>\n",
       "      <th>pose_x0</th>\n",
       "      <th>pose_y0</th>\n",
       "      <th>pose_z0</th>\n",
       "      <th>pose_x1</th>\n",
       "      <th>pose_y1</th>\n",
       "      <th>pose_z1</th>\n",
       "      <th>pose_x2</th>\n",
       "      <th>...</th>\n",
       "      <th>left_hand_z17</th>\n",
       "      <th>left_hand_x18</th>\n",
       "      <th>left_hand_y18</th>\n",
       "      <th>left_hand_z18</th>\n",
       "      <th>left_hand_x19</th>\n",
       "      <th>left_hand_y19</th>\n",
       "      <th>left_hand_z19</th>\n",
       "      <th>left_hand_x20</th>\n",
       "      <th>left_hand_y20</th>\n",
       "      <th>left_hand_z20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pen</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.411205</td>\n",
       "      <td>0.171439</td>\n",
       "      <td>-0.444944</td>\n",
       "      <td>0.427558</td>\n",
       "      <td>0.101079</td>\n",
       "      <td>-0.442622</td>\n",
       "      <td>0.442020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034374</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>0.782419</td>\n",
       "      <td>-0.050452</td>\n",
       "      <td>0.351776</td>\n",
       "      <td>0.811713</td>\n",
       "      <td>-0.058446</td>\n",
       "      <td>0.355583</td>\n",
       "      <td>0.840286</td>\n",
       "      <td>-0.062885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pen</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.409809</td>\n",
       "      <td>0.172871</td>\n",
       "      <td>-0.436721</td>\n",
       "      <td>0.425809</td>\n",
       "      <td>0.101436</td>\n",
       "      <td>-0.434942</td>\n",
       "      <td>0.440617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pen</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.408941</td>\n",
       "      <td>0.176701</td>\n",
       "      <td>-0.447924</td>\n",
       "      <td>0.424350</td>\n",
       "      <td>0.102597</td>\n",
       "      <td>-0.446523</td>\n",
       "      <td>0.439302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041063</td>\n",
       "      <td>0.350450</td>\n",
       "      <td>0.785777</td>\n",
       "      <td>-0.057279</td>\n",
       "      <td>0.356252</td>\n",
       "      <td>0.815158</td>\n",
       "      <td>-0.064492</td>\n",
       "      <td>0.362689</td>\n",
       "      <td>0.842664</td>\n",
       "      <td>-0.068444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pen</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.408325</td>\n",
       "      <td>0.181097</td>\n",
       "      <td>-0.382025</td>\n",
       "      <td>0.423258</td>\n",
       "      <td>0.104253</td>\n",
       "      <td>-0.384934</td>\n",
       "      <td>0.438338</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051957</td>\n",
       "      <td>0.354071</td>\n",
       "      <td>0.783193</td>\n",
       "      <td>-0.066484</td>\n",
       "      <td>0.361161</td>\n",
       "      <td>0.810169</td>\n",
       "      <td>-0.070669</td>\n",
       "      <td>0.369577</td>\n",
       "      <td>0.833266</td>\n",
       "      <td>-0.072963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pen</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.406662</td>\n",
       "      <td>0.182538</td>\n",
       "      <td>-0.425791</td>\n",
       "      <td>0.420278</td>\n",
       "      <td>0.104820</td>\n",
       "      <td>-0.424994</td>\n",
       "      <td>0.435957</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052415</td>\n",
       "      <td>0.352663</td>\n",
       "      <td>0.778692</td>\n",
       "      <td>-0.069039</td>\n",
       "      <td>0.359794</td>\n",
       "      <td>0.805952</td>\n",
       "      <td>-0.075388</td>\n",
       "      <td>0.368541</td>\n",
       "      <td>0.831612</td>\n",
       "      <td>-0.079067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  accuracy  sequence   pose_x0   pose_y0   pose_z0   pose_x1   pose_y1  \\\n",
       "0   pen         1         0  0.411205  0.171439 -0.444944  0.427558  0.101079   \n",
       "1   pen         1         0  0.409809  0.172871 -0.436721  0.425809  0.101436   \n",
       "2   pen         1         0  0.408941  0.176701 -0.447924  0.424350  0.102597   \n",
       "3   pen         1         0  0.408325  0.181097 -0.382025  0.423258  0.104253   \n",
       "4   pen         1         0  0.406662  0.182538 -0.425791  0.420278  0.104820   \n",
       "\n",
       "    pose_z1   pose_x2  ...  left_hand_z17  left_hand_x18  left_hand_y18  \\\n",
       "0 -0.442622  0.442020  ...      -0.034374       0.348000       0.782419   \n",
       "1 -0.434942  0.440617  ...       0.000000       0.000000       0.000000   \n",
       "2 -0.446523  0.439302  ...      -0.041063       0.350450       0.785777   \n",
       "3 -0.384934  0.438338  ...      -0.051957       0.354071       0.783193   \n",
       "4 -0.424994  0.435957  ...      -0.052415       0.352663       0.778692   \n",
       "\n",
       "   left_hand_z18  left_hand_x19  left_hand_y19  left_hand_z19  left_hand_x20  \\\n",
       "0      -0.050452       0.351776       0.811713      -0.058446       0.355583   \n",
       "1       0.000000       0.000000       0.000000       0.000000       0.000000   \n",
       "2      -0.057279       0.356252       0.815158      -0.064492       0.362689   \n",
       "3      -0.066484       0.361161       0.810169      -0.070669       0.369577   \n",
       "4      -0.069039       0.359794       0.805952      -0.075388       0.368541   \n",
       "\n",
       "   left_hand_y20  left_hand_z20  \n",
       "0       0.840286      -0.062885  \n",
       "1       0.000000       0.000000  \n",
       "2       0.842664      -0.068444  \n",
       "3       0.833266      -0.072963  \n",
       "4       0.831612      -0.079067  \n",
       "\n",
       "[5 rows x 228 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 9: Delete not important data\n",
    "data = pd.read_csv('combined_coordinates.csv')\n",
    "\n",
    "# Identify columns to remove: include specific ranges and all visibility points\n",
    "###columns_to_remove_1 = [f\"pose_{c}{i}\" for c in ['x', 'y', 'z', 'v'] for i in range(0, 11)]\n",
    "###columns_to_remove_2 = [f\"pose_{c}{i}\" for c in ['x', 'y', 'z', 'v'] for i in range(23, 33)]\n",
    "visibility_columns_pose = [col for col in data.columns if 'pose_v' in col]\n",
    "visibility_columns_left_hand = [col for col in data.columns if 'left_hand_v' in col]\n",
    "visibility_columns_right_hand = [col for col in data.columns if 'right_hand_v' in col]\n",
    "\n",
    "# Combine all columns to remove\n",
    "###columns_to_remove = columns_to_remove_1 + columns_to_remove_2 + visibility_columns_pose + visibility_columns_left_hand + visibility_columns_right_hand\n",
    "columns_to_remove = visibility_columns_pose + visibility_columns_left_hand + visibility_columns_right_hand\n",
    "columns_to_remove = [col for col in columns_to_remove if col in data.columns]\n",
    "\n",
    "# Drop the selected columns from the dataframe\n",
    "data_filtered = data.drop(columns=columns_to_remove)\n",
    "\n",
    "# Round the values and save the filtered data\n",
    "###data_filtered = data_filtered.round(3)\n",
    "data_filtered.to_csv('filtered_coordinates.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the filtered dataset\n",
    "data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_merge_columns(file_path, output_file_path):\n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    data['accuracy'] = data['accuracy'].map({0: 'W', 1: 'R'})\n",
    "    data['label'] = data.iloc[:, 0].astype(str) + '_' + data['accuracy']\n",
    "    \n",
    "    label_column = data.pop('label') \n",
    "    data.insert(0, 'label', label_column)\n",
    "    \n",
    "    data.drop(columns=['class', 'accuracy'], inplace=True)\n",
    "    data.to_csv(output_file_path, index=False)\n",
    "\n",
    "input_file_path = 'filtered_coordinates.csv'  \n",
    "output_file_path = 'modified_file.csv'  \n",
    "transform_and_merge_columns(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Create folders in directory\n",
    "data = pd.read_csv('filtered_coordinates.csv')\n",
    "\n",
    "base_dir = 'DataBase'\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "unique_classes = data['class'].unique()\n",
    "\n",
    "for class_value in unique_classes:\n",
    "    class_dir = os.path.join(base_dir, str(class_value))\n",
    "    if not os.path.exists(class_dir):\n",
    "        os.makedirs(class_dir)\n",
    "    \n",
    "    class_data = data[data['class'] == class_value]\n",
    "\n",
    "    for accuracy_type, subfolder_name in zip([1, 0], ['R', 'W']):\n",
    "        accuracy_dir = os.path.join(class_dir, subfolder_name)\n",
    "        if not os.path.exists(accuracy_dir):\n",
    "            os.makedirs(accuracy_dir)\n",
    "        \n",
    "        accuracy_data = class_data[class_data['accuracy'] == accuracy_type]\n",
    "        \n",
    "        unique_sequences = accuracy_data['sequence'].unique()\n",
    "        \n",
    "        for sequence in unique_sequences:\n",
    "            sequence_dir = os.path.join(accuracy_dir, str(sequence))\n",
    "            if not os.path.exists(sequence_dir):\n",
    "                os.makedirs(sequence_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Save individual np arrays\n",
    "data = pd.read_csv('filtered_coordinates.csv')\n",
    "base_dir = 'DataBase'\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "for class_value in data['class'].unique():\n",
    "    class_dir = os.path.join(base_dir, str(class_value))\n",
    "    class_data = data[data['class'] == class_value]\n",
    "\n",
    "    for accuracy_type, subfolder_name in zip([1, 0], ['R', 'W']):\n",
    "        accuracy_dir = os.path.join(class_dir, subfolder_name)\n",
    "        accuracy_data = class_data[class_data['accuracy'] == accuracy_type]\n",
    "\n",
    "        for sequence in accuracy_data['sequence'].unique():\n",
    "            sequence_dir = os.path.join(accuracy_dir, str(sequence))\n",
    "            sequence_data = accuracy_data[accuracy_data['sequence'] == sequence]\n",
    "\n",
    "            sequence_data = sequence_data.reset_index(drop=True)\n",
    "\n",
    "            for index, row in sequence_data.iterrows():\n",
    "                frame_path = os.path.join(sequence_dir, f'{index}.npy')\n",
    "                np.save(frame_path, row.values[3:])\n",
    "\n",
    "print(\"cosas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Test if array is in the correct way\n",
    "np.load('DataBase/scissors/W/4/0.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array=np.load('DataBase/movement_1/W/0/0.npy', allow_pickle=True)\n",
    "len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Install new libraries for processing and training\n",
    "!pip install tensorflow \n",
    "!pip install tensorflow-macos\n",
    "!pip install tensorflow-metal\n",
    "!pip install numpy --upgrade\n",
    "!pip install pandas --upgrade\n",
    "!pip install matplotlib --upgrade\n",
    "!pip install scikit-learn --upgrade\n",
    "!pip install scipy --upgrade\n",
    "!pip install plotly --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y apple tensorflow-deps\n",
    "!conda install notebook -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "\n",
    "print (f\"Python Platform: {platform.platform ()}\")\n",
    "print (f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print ()\n",
    "\n",
    "print (f\"Python {sys.version}\")\n",
    "print (f\"Pandas {pd.__version__}\")\n",
    "print (f\"Scikit-Learn {sk.__version__}\")\n",
    "print (f\"SciPy {sp.__version__}\")\n",
    "gpu = len (tf.config.list_physical_devices ('GPU'))>0\n",
    "print (\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Import new libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Joining and labeling arrays\n",
    "base_dir = 'DataBase'\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for class_name in os.listdir(base_dir):\n",
    "    class_dir = os.path.join(base_dir, class_name)\n",
    "    for accuracy_type in ['R', 'W']:\n",
    "        accuracy_dir = os.path.join(class_dir, accuracy_type)\n",
    "        for sequence in os.listdir(accuracy_dir):\n",
    "            sequence_dir = os.path.join(accuracy_dir, sequence)\n",
    "            sequence_features = []\n",
    "            for file_name in os.listdir(sequence_dir):\n",
    "                file_path = os.path.join(sequence_dir, file_name)\n",
    "                data = np.load(file_path, allow_pickle=True)\n",
    "                sequence_features.append(data)\n",
    "            features.append(sequence_features)\n",
    "            labels.append(f\"{class_name}_{accuracy_type}\")\n",
    "\n",
    "features_padded = pad_sequences(features, padding='post', dtype='float32')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell after 15: Extract and print unique labels before encoding\n",
    "# This code assumes 'labels' list is already populated as in Cell 15\n",
    "\n",
    "# Extract unique labels\n",
    "unique_labels = set(labels)\n",
    "\n",
    "# Print unique labels\n",
    "print(\"Unique labels before encoding:\")\n",
    "print(unique_labels)\n",
    "\n",
    "# Optionally, if you want to save these unique labels to a text file:\n",
    "with open('unique_labels.txt', 'w') as f:\n",
    "    for label in sorted(unique_labels):  # Sorting to maintain consistent order\n",
    "        f.write(\"%s\\n\" % label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Labels test (features_padded and labels_encoded) outputs expected in .shape f_p(x, y, z) / l_e(x, )\n",
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Set X and y\n",
    "X = np.array(features_padded)\n",
    "y = to_categorical(labels_encoded).astype(int)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume X is your existing NumPy array\n",
    "# Example: X = np.random.rand(10, 10, 10) - Replace with your actual array\n",
    "\n",
    "# Set numpy print options to avoid truncation\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# Convert the array X to string\n",
    "array_str = np.array2string(X)\n",
    "\n",
    "# Write the string to a file\n",
    "with open('X.txt', 'w') as f:\n",
    "    f.write(array_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y.txt', 'w') as file:\n",
    "    # Convert the list into a string and write it to the file\n",
    "    file.write(str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Split values for testing and training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model + LSTM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Import training dependencies\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: To web monitor live training later\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6  # Put here amount of movements you want to detect\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(68, 162)))  # Put here las two values of 'X.shape' in this case X.shape = (122, 68, 162)\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='tanh')) \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [0.7, 0.3, 0.5, 0.4, 0.3, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_padded[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep track of the training in your terminal cd to Logs/train folder and then write tensorboard --logdir=. a link will pop up that you just need to put in your search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have a single set of features to predict\n",
    "model_output = model.predict(np.expand_dims(features_padded[100], axis=0))  \n",
    "predicted_index = np.argmax(model_output[0])  \n",
    "predicted_label = label_encoder.inverse_transform([predicted_index])[0]  \n",
    "\n",
    "print(\"Predicted Label:\", predicted_label)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('actions.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()  \n",
    "yhat = np.argmax(yhat, axis=1).tolist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real time predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the contents of the text file\n",
    "with open('unique_labels.txt', 'r') as file:\n",
    "    actions = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Print the list of actions\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Number of frames to collect data for\n",
    "num_frames_to_collect = 68\n",
    "\n",
    "# Initialize an array to store the collected data\n",
    "collected_data = []\n",
    "\n",
    "# Setup MediaPipe instances\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture\")\n",
    "else:\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose, \\\n",
    "         mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5, max_num_hands=2) as hands:\n",
    "        frame_count = 0\n",
    "        while cap.isOpened() and frame_count < num_frames_to_collect:\n",
    "            ret, image = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Unable to read frame from video capture\")\n",
    "                break\n",
    "\n",
    "            # Flip image to simulate mirror view\n",
    "            image = cv2.flip(image, 1)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Make detections\n",
    "            pose_results = pose.process(image)\n",
    "            hand_results = hands.process(image)\n",
    "\n",
    "            # Flatten pose landmarks, skipping certain indices\n",
    "            if pose_results.pose_landmarks:\n",
    "                pose_landmarks = [\n",
    "                    round(value, 3) for idx, landmark in enumerate(pose_results.pose_landmarks.landmark)\n",
    "                    if not (0 <= idx <= 11 or 23 <= idx <= 32)  # Adjust to 0-based indexing and include index 32\n",
    "                    for value in (landmark.x, landmark.y, landmark.z)\n",
    "                ]\n",
    "            else:\n",
    "                pose_landmarks = [0] * (10 * 3)  # Adjust count to reflect the remaining landmarks\n",
    "\n",
    "            # Initialize hand landmarks placeholders\n",
    "            right_hand_landmarks = [0] * (21 * 3)\n",
    "            left_hand_landmarks = [0] * (21 * 3)\n",
    "\n",
    "            # Detect and sort hand landmarks\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                hand_labels = [hand.classification[0].label for hand in hand_results.multi_handedness]\n",
    "                for hand_landmarks, label in zip(hand_results.multi_hand_landmarks, hand_labels):\n",
    "                    flat_hand = [round(value, 3) for landmark in hand_landmarks.landmark\n",
    "                                 for value in (landmark.x, landmark.y, landmark.z)]\n",
    "                    if label == 'Right':\n",
    "                        right_hand_landmarks = flat_hand\n",
    "                    else:\n",
    "                        left_hand_landmarks = flat_hand\n",
    "\n",
    "            # Combine all landmarks\n",
    "            all_landmarks = pose_landmarks + right_hand_landmarks + left_hand_landmarks\n",
    "            collected_data.extend(all_landmarks)  # Append to flat list\n",
    "\n",
    "            # Drawing\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2),\n",
    "                )\n",
    "\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                        mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                    )\n",
    "\n",
    "            cv2.imshow(\"Raw Webcam Feed with Landmarks\", image)\n",
    "\n",
    "            # Increment frame count\n",
    "            frame_count += 1\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Convert to numpy array\n",
    "collected_data_array = np.array(collected_data)\n",
    "print(\"Data shape:\", collected_data_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put time to wait\n",
    "# Import model\n",
    "# make predictions straight to matrix \n",
    "# Convert matrix results into labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your model\n",
    "model = load_model('actions.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your model\n",
    "model = load_model('actions.h5')\n",
    "\n",
    "# Example data for one sequence\n",
    "# Assume 'sequence' is a list of arrays, with each array representing frame data which is then flattened\n",
    "sequence = collected_data_array  # Replace these with actual frame data\n",
    "\n",
    "# If your model expects a 2D array (batch size, features):\n",
    "sequence = np.expand_dims(sequence, axis=0)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(sequence)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynewv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
